{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparkをはじめてみよう\n",
    "Sparkを使って、README.mdを読み込み、いろいろと操作してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scはSparkContextといって、sparkの基本となるオブジェクト。\n",
    "textFile = sc.textFile(\"../README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 行数をカウント\n",
    "textFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# \"Spark\"を含む行を抽出。\n",
    "sparkLines = textFile.filter(lambda l: \"Spark\" in l)\n",
    "sparkLines.count()\n",
    "\n",
    "### filter-> pythonのfilterと同じ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# \"Spark\"を含む行を全件取得\n",
    "sparkLines.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 出現単語ごとの件数を取得\n",
    "textFile.flatMap(\n",
    "    lambda l: l.strip().split()  # 行を単語に分解\n",
    ").map(\n",
    "    lambda w: (w, 1)  # 単語の数をカウントするため、（単語、1）というタプルに変換\n",
    ").reduceByKey(\n",
    "    lambda x, y: x + y  # 単語ごとに集計\n",
    ").collect()\n",
    "\n",
    "### flatMap -> mapしつつ、内部のコレクションを展開する関数。[[1,2], [3,4], [4,5]]をflatMapすると、[1,2,3,4,5]になる。\n",
    "### map -> pythonのmapと同じ\n",
    "### reduceByKey -> (key, value)のコレクションに対して、キーごとに値を集計する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDDの作り方\n",
    "RDDの作り方は、大きく分けて２通りあります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# コレクションオブジェクトから作成する\n",
    "l = [1, 2, 3]\n",
    "rdd = sc.parallelize(l)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 外部から読み込む\n",
    "rdd = sc.textFile(\"../README.md\")\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDDの処理イメージ\n",
    "RDDのTransformationとActionがどのように実行されるのか見てみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# テキストファイルからRDD作成。この時点ではRDDオブジェクトが作成されるだけで、実際の読み込みは発生しない。\n",
    "linesRDD = sc.textFile(\"../README.md\")\n",
    "print type(linesRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# \"Spark\"というワードを持つ行のみ抽出。ここでも新たなRDDオブジェクトが作成されるだけ。\n",
    "filteredRDD =  linesRDD.filter(lambda l: \"Spark\" in l)\n",
    "print type(filteredRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 行数取得。ここでやっとテキストファイルから読み込んでフィルタするという操作が実行される。\n",
    "cnt = filteredRDD.count()\n",
    "print cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDDのキャッシュ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textFile = sc.textFile(\"../README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cache()を呼び出すことで、メモリにキャッシュすることが指示されます。ここではまだキャッシュされません。\n",
    "cached = textFile.filter(lambda l: \"Spark\" in l).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ここで、テキストファイルからの読み込みとフィルタ処理が行われた上で、結果がメモリにキャッシュされます。\n",
    "cached.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# メモリからfilter処理までが終わった状態のデータを読み込み、処理します。テキストファイルが再び読み込まれることはありません。\n",
    "import datetime\n",
    "cached.saveAsTextFile(\"README-filtered-{}.md\".format(datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Sparkの管理画面で実行を確認する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffleのないケース"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.textFile(\"../licenses/*.txt\").filter(lambda l: \"Copyright\" in l).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[管理画面で確認](http://localhost:4040/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffleのあるケース"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc.textFile(\"../licenses/*.txt\").flatMap(lambda l: l.split()).map(lambda w: (w, 1)).reduceByKey(lambda x, y: x + y).take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[管理画面で確認](http://localhost:4040/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrameを動かしてみよう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DataFrameを使うためのエントリーポイントとなるオブジェクト作成\n",
    "from pyspark.sql import SparkSession\n",
    "spark =  SparkSession.builder.appName(\"pycon2016\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 下記のようなjsonがある\n",
    "%cat ../examples/src/main/resources/people.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# これを読み込む\n",
    "df = spark.read.json(\"../examples/src/main/resources/people.json\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 特定のカラムを選択\n",
    "df.select(\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 絞り込み選択\n",
    "df.filter(df['age'] > 21).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 集約関数\n",
    "df.groupBy(\"age\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# メモリ上にテンポラリテーブルとして保存することも可能。\n",
    "df.createOrReplaceTempView(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# テーブルには直接SQLを発行することができます。戻り値はDataFrame。\n",
    "res = spark.sql(\"select age from people where name = 'Andy'\")\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# なお、スキーマについてはデータから類数されています。\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# もちろん、データを書き込むことも可能。jsonで読んだデータをcsvで書き込む。\n",
    "newDf = df.select(df[\"name\"], df[\"age\"] + 10)\n",
    "newDf.write.mode(\"overwrite\").csv(\"/tmp/new_people.csv\")\n",
    "spark.read.csv(\"/tmp/new_people.csv\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 様々なデータソース"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# csv\n",
    "dfCsv = spark.read.csv(\"../examples/src/main/resources/test.csv\")\n",
    "dfCsv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# parquet\n",
    "dfParquet = spark.read.parquet(\"../examples/src/main/resources/users.parquet\")\n",
    "dfParquet.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrameを使うメリット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "import pyspark.sql.functions as func\n",
    "\n",
    "# 元データ\n",
    "# (名前、部署名、年齢)\n",
    "raw = [(\"Pete\", \"dev\", 20), (\"Keith\", \"dev\", 22),  (\"Roger\", \"sales\", 30), (\"John\", \"sales\", 28)]\n",
    "rdd = sc.parallelize(raw)\n",
    "df = spark.createDataFrame(\n",
    "    rdd.map(\n",
    "        lambda r: Row(name=r[0], department=r[1], age=r[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RDDで処理するケース\n",
    "rdd.map(\n",
    "    lambda r: (r[1], (float(r[2]), 1))  # (部署名、(年齢、カウント))のタプルに変換\n",
    ").reduceByKey(\n",
    "    lambda x, y: (x[0] + y[0], x[1] + y[1])  # 部署ごとに年齢と人数の合計を算出\n",
    ").mapValues(\n",
    "    lambda v: v[0] / v[1]  # 年齢の合計と人数の合計割り算\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DataFrameで処理するケース\n",
    "df.groupBy(\"department\").agg(func.avg(\"age\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## メールのスパム判定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# まずは元データの中身を見る。\n",
    "sc.textFile(\"../data/smsspam/SMSSpamCollection\").take(3)\n",
    "\n",
    "# ラベルと文章からなるtsvファイル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# オリジナルデータをロード\n",
    "# スキーマを個別で指定したい場合は、以下のようにStructTypeを作成する。\n",
    "schema = StructType([StructField(\"label\", StringType(), True), StructField(\"body\", StringType(), True)])\n",
    "df = spark.read.csv(\"../data/smsspam/SMSSpamCollection\", schema=schema, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# ham/spamといったラベルを0/1に変換\n",
    "labeled = df.select(\n",
    "    when(df[\"label\"] == \"ham\", 0).otherwise(1)\n",
    "    .cast(IntegerType())\n",
    "    .alias(\"label\"),\n",
    "    df[\"body\"])\n",
    "\n",
    "# トレーニングデータとテストデータに分割\n",
    "training, test = labeled.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, Tokenizer, StopWordsRemover\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# 以下の流れのパイプラインを構築する。\n",
    "# 文章を単語に分割 => 単語の選別 => 単語の出現頻度カウント => ロジスティック回帰モデル作成\n",
    "\n",
    "# まずは、パイプラインのそれぞれのステージを構築するオブジェクトを作成する。\n",
    "## 文章を単語に分割するやつ\n",
    "tokenizer = Tokenizer(inputCol=\"body\", outputCol=\"words\")\n",
    "\n",
    "## 単語を選別するやつ\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"filtered\")\n",
    "\n",
    "## 単語を数えるやつ\n",
    "hashingTF = HashingTF(inputCol=remover.getOutputCol(), outputCol=\"features\")\n",
    "\n",
    "## ロジスティック回帰モデルを作成するやつ\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 上記のオブジェクトを組み合わせてパイプラインを構築。\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 学習モデル構築\n",
    "model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# モデルの精度（AUC）を計測\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "prediction = model.transform(test)\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "evaluator.evaluate(prediction)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
