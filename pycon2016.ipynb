{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparkをはじめてみよう\n",
    "Sparkを使って、README.mdを読み込み、いろいろと操作してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textFile = sc.textFile(\"../README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 行数をカウント\n",
    "textFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# \"Spark\"を含む行を抽出。\n",
    "sparkLines = textFile.filter(lambda l: \"Spark\" in l)\n",
    "sparkLines.count()\n",
    "\n",
    "### filter-> pythonのfilterと同じ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# \"Spark\"を含む行を全件取得\n",
    "sparkLines.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 出現単語ごとの件数を取得\n",
    "textFile.flatMap(lambda l: l.strip().split()).map(lambda w: (w, 1)).reduceByKey(lambda x, y: x + y).collect()\n",
    "\n",
    "### flatMap -> mapしつつ、内部のコレクションを展開する関数。[[1,2], [3,4], [4,5]]をflatMapすると、[1,2,3,4,5]になる。\n",
    "### map -> pythonのmapと同じ\n",
    "### reduceByKey -> (key, value)のコレクションに対して、キーごとに値を集計する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDDの作り方\n",
    "RDDの作り方は、大きく分けて２通りあります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# コレクションオブジェクトから作成する\n",
    "l = [1, 2, 3]\n",
    "rdd = sc.parallelize(l)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 外部から読み込む\n",
    "rdd = sc.textFile(\"../README.md\")\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDDの処理イメージ\n",
    "RDDのTransformationとActionがどのように実行されるのか見てみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# テキストファイルからRDD作成。この時点ではRDDオブジェクトが作成されるだけで、実際の読み込みは発生しない。\n",
    "linesRDD = sc.textFile(\"../README.md\")\n",
    "print type(linesRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# \"Spark\"というワードを持つ行のみ抽出。ここでも新たなRDDオブジェクトが作成されるだけ。\n",
    "filteredRDD =  linesRDD.filter(lambda l: \"Spark\" in l)\n",
    "print type(filteredRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 行数取得。ここでやっとテキストファイルから読み込んでフィルタするという操作が実行される。\n",
    "filteredRDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDDのキャッシュ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textFile = sc.textFile(\"../README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cache()を呼び出すことで、メモリにキャッシュすることが指示されます。ここではまだキャッシュされません。\n",
    "cached = textFile.filter(lambda l: \"Spark\" in l).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ここで、テキストファイルからの読み込みとフィルタ処理が行われた上で、結果がメモリにキャッシュされます。\n",
    "cached.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# メモリからfilter処理までが終わった状態のデータを読み込み、処理します。テキストファイルが再び読み込まれることはありません。\n",
    "import datetime\n",
    "cached.saveAsTextFile(\"README-filtered-{}.md\".format(datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Sparkの管理画面で実行を確認する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffleのないケース"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.textFile(\"../licenses/*.txt\").filter(lambda l: \"Copyright\" in l).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[管理画面で確認](http://localhost:4040/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffleのあるケース"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc.textFile(\"../licenses/*.txt\").flatMap(lambda l: l.split()).map(lambda w: (w, 1)).reduceByKey(lambda x, y: x + y).take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[管理画面で確認](http://localhost:4040/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrameを動かしてみよう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DataFrameを使うためのエントリーポイントとなるオブジェクト作成\n",
    "from pyspark.sql import SparkSession\n",
    "spark =  SparkSession.builder.appName(\"pycon2016\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 下記のようなjsonがあります。\n",
    "%cat ../examples/src/main/resources/people.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# これを読み込みます。\n",
    "df = spark.read.json(\"../examples/src/main/resources/people.json\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 特定のカラムを選択します。\n",
    "df.select(\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 絞り込み選択\n",
    "df.filter(df['age'] > 21).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 集約関数\n",
    "df.groupBy(\"age\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# メモリ上にテンポラリテーブルとして保存することもできます。\n",
    "df.createOrReplaceTempView(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# テーブルには直接SQLを発行することができます。戻り値はDataFrameです。\n",
    "res = spark.sql(\"select age from people where name = 'Andy'\")\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# なお、スキーマについてはデータから類数されています。\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# もちろん、データを書き込むことも可能です。jsonで読んだデータをcsvで書き込んでみます。\n",
    "df = spark.read.json(\"../examples/src/main/resources/people.json\")\n",
    "df.select(df[\"name\"], df[\"age\"] + 10).write.mode(\"overwrite\").csv(\"/tmp/new_people.csv\")\n",
    "spark.read.csv(\"/tmp/new_people.csv\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 様々なデータソース"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# csv\n",
    "dfCsv = spark.read.csv(\"../examples/src/main/resources/test.csv\")\n",
    "dfCsv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# parquet\n",
    "dfParquet = spark.read.parquet(\"../examples/src/main/resources/users.parquet\")\n",
    "dfParquet.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrameを使うメリット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "import pyspark.sql.functions as func\n",
    "\n",
    "# 元データ\n",
    "raw = [(\"Pete\", \"dev\", 20), (\"Keith\", \"dev\", 22),  (\"Roger\", \"sales\", 30), (\"John\", \"sales\", 28)]\n",
    "rdd = sc.parallelize(raw)\n",
    "df = spark.createDataFrame(sc.parallelize(raw).map(lambda r: Row(name=r[0], department=r[1], age=r[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# RDDで処理するケース\n",
    "rdd.map(lambda r: (r[1], (float(r[2]), 1))).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])).mapValues(lambda v: v[0] / v[1]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DataFrameで処理するケース\n",
    "df.groupBy(\"department\").agg(func.avg(\"age\")).show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
