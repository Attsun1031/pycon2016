{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparkをはじめてみよう\n",
    "Sparkを使って、README.mdを読み込み、いろいろと操作してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scはSparkContextといって、sparkの基本となるオブジェクト。\n",
    "textFile = sc.textFile(\"../README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 行数をカウント\n",
    "textFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"Spark\"を含む行を抽出。\n",
    "sparkLines = textFile.filter(lambda l: \"Spark\" in l)\n",
    "sparkLines.count()\n",
    "\n",
    "### filter-> pythonのfilterと同じ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'# Apache Spark',\n",
       " u'Spark is a fast and general cluster computing system for Big Data. It provides',\n",
       " u'rich set of higher-level tools including Spark SQL for SQL and DataFrames,']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"Spark\"を含む行を3行取得\n",
    "sparkLines.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'when', 1),\n",
       " (u'R,', 1),\n",
       " (u'including', 3),\n",
       " (u'computation', 1),\n",
       " (u'using:', 1),\n",
       " (u'guidance', 2),\n",
       " (u'Scala,', 1),\n",
       " (u'environment', 1),\n",
       " (u'only', 1),\n",
       " (u'rich', 1)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 出現単語ごとの件数を取得\n",
    "textFile.flatMap(\n",
    "    lambda l: l.strip().split()  # 行を単語に分解\n",
    ").map(\n",
    "    lambda w: (w, 1)  # 単語の数をカウントするため、（単語、1）というタプルに変換\n",
    ").reduceByKey(\n",
    "    lambda x, y: x + y  # 単語ごとに集計\n",
    ").take(10)\n",
    "\n",
    "### flatMap -> mapしつつ、内部のコレクションを展開する関数。[[1,2], [3,4], [4,5]]をflatMapすると、[1,2,3,4,5]になる。\n",
    "### map -> pythonのmapと同じ\n",
    "### reduceByKey -> (key, value)のコレクションに対して、キーごとに値を集計する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDDの処理イメージ\n",
    "RDDのTransformationとActionがどのように実行されるのか見てみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "# テキストファイルからRDD作成。この時点ではRDDオブジェクトが作成されるだけで、実際の読み込みは発生しない。\n",
    "linesRDD = sc.textFile(\"../README.md\")\n",
    "print type(linesRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n"
     ]
    }
   ],
   "source": [
    "# \"Spark\"というワードを持つ行のみ抽出。ここでも新たなRDDオブジェクトが作成されるだけ。\n",
    "filteredRDD =  linesRDD.filter(lambda l: \"Spark\" in l)\n",
    "print type(filteredRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "# 行数取得。ここでやっとテキストファイルから読み込んでフィルタするという操作が実行される。\n",
    "cnt = filteredRDD.count()\n",
    "print cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrameを動かしてみよう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DataFrameを使うためのエントリーポイントとなるオブジェクト作成\n",
    "from pyspark.sql import SparkSession\n",
    "spark =  SparkSession.builder.appName(\"pycon2016\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\":\"Michael\"}\r\n",
      "{\"name\":\"Andy\", \"age\":30}\r\n",
      "{\"name\":\"Justin\", \"age\":19}\r\n"
     ]
    }
   ],
   "source": [
    "# 下記のようなjsonがある\n",
    "%cat ../examples/src/main/resources/people.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# これを読み込む\n",
    "df = spark.read.json(\"../examples/src/main/resources/people.json\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 特定のカラムを選択\n",
    "df.select(\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 絞り込み選択\n",
    "df.filter(df['age'] > 21).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|  19|    1|\n",
      "|null|    1|\n",
      "|  30|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 集約関数\n",
    "df.groupBy(\"age\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# メモリ上にテンポラリテーブルとして保存することも可能。\n",
    "df.createOrReplaceTempView(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|age|\n",
      "+---+\n",
      "| 30|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# テーブルには直接SQLを発行することができます。戻り値はDataFrame。\n",
    "res = spark.sql(\"select age from people where name = 'Andy'\")\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# もちろん、データを書き込むことも可能。jsonで読んだデータをcsvで書き込む。\n",
    "newDf = df.select(df[\"name\"], df[\"age\"] + 10)\n",
    "newDf.write.mode(\"overwrite\").csv(\"/tmp/new_people.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|    _c0|_c1|\n",
      "+-------+---+\n",
      "|Michael|   |\n",
      "|   Andy| 40|\n",
      "| Justin| 29|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv(\"/tmp/new_people.csv\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrameを使うメリット\n",
    "名前、部署、年齢からなる社員名簿から部署ごとの平均年齢をとる例を実装してみる。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "import pyspark.sql.functions as func\n",
    "\n",
    "# 元データ\n",
    "# (名前、部署名、年齢)\n",
    "raw = [(\"Pete\", \"dev\", 20), (\"Keith\", \"dev\", 22),  (\"Roger\", \"sales\", 30), (\"John\", \"sales\", 28)]\n",
    "\n",
    "# RDD版\n",
    "rdd = sc.parallelize(raw)\n",
    "\n",
    "# DataFrame版\n",
    "df = spark.createDataFrame(\n",
    "    rdd.map(\n",
    "        lambda r: Row(name=r[0], department=r[1], age=r[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sales', 29.0), ('dev', 21.0)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDDで処理するケース\n",
    "rdd.map(\n",
    "    lambda r: (r[1], (float(r[2]), 1))  # (部署名、(年齢、カウント))のタプルに変換\n",
    ").reduceByKey(\n",
    "    lambda x, y: (x[0] + y[0], x[1] + y[1])  # 部署ごとに年齢と人数の合計を算出\n",
    ").mapValues(\n",
    "    lambda v: v[0] / v[1]  # 年齢の合計と人数の合計割り算\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|department|avg(age)|\n",
      "+----------+--------+\n",
      "|       dev|    21.0|\n",
      "|     sales|    29.0|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrameで処理するケース\n",
    "df.groupBy(\"department\").agg(func.avg(\"age\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## メールのスパム判定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'ham\\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...',\n",
       " u'ham\\tOk lar... Joking wif u oni...',\n",
       " u\"spam\\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\"]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# まずは元データの中身を見る。\n",
    "sc.textFile(\"../data/smsspam/SMSSpamCollection\").take(3)\n",
    "\n",
    "# ラベルと文章からなるtsvファイル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# オリジナルデータをロード\n",
    "# スキーマを個別で指定したい場合は、以下のようにStructTypeを作成する。\n",
    "schema = StructType([StructField(\"label\", StringType(), True), StructField(\"body\", StringType(), True)])\n",
    "df = spark.read.csv(\"../data/smsspam/SMSSpamCollection\", schema=schema, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# ham/spamといったラベルを0/1に変換\n",
    "labeled = df.select(\n",
    "    when(df[\"label\"] == \"ham\", 0).otherwise(1)\n",
    "    .cast(IntegerType())\n",
    "    .alias(\"label\"),\n",
    "    df[\"body\"])\n",
    "\n",
    "# トレーニングデータとテストデータに分割\n",
    "training, test = labeled.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, Tokenizer, StopWordsRemover\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# 以下の流れのパイプラインを構築する。\n",
    "# 文章を単語に分割 => 単語の選別 => 単語の出現頻度カウント => ロジスティック回帰モデル作成\n",
    "\n",
    "# まずは、パイプラインのそれぞれのステージを構築するオブジェクトを作成する。\n",
    "## 文章を単語に分割するやつ\n",
    "tokenizer = Tokenizer(inputCol=\"body\", outputCol=\"words\")\n",
    "\n",
    "## 単語を選別するやつ\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"filtered\")\n",
    "\n",
    "## 単語を数えるやつ\n",
    "hashingTF = HashingTF(inputCol=remover.getOutputCol(), outputCol=\"features\")\n",
    "\n",
    "## ロジスティック回帰モデルを作成するやつ\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 上記のオブジェクトを組み合わせてパイプラインを構築。\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 学習モデル構築\n",
    "model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9805776464864642"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# モデルの精度（AUC）を計測\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "prediction = model.transform(test)\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "evaluator.evaluate(prediction)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
